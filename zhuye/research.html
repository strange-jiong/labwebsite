<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>

<!-- Mirrored from www.iro.umontreal.ca/~bengioy/yoshua_en/research.html by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 14 May 2016 07:31:47 GMT -->
<head>
<!-- #BeginTemplate "/Templates/people_faculty_profile_new.dwt" --><!-- DW6 -->
  <meta name="ROBOTS" content="NOARCHIVE">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <link rel="icon" href="http://www.gsd.harvard.edu/favicon.ico"
 type="image/x-icon">
  <link rel="shortcut icon"
 href="http://www.gsd.harvard.edu/favicon.ico">
  <link rel="stylesheet" type="text/css"
 href="research_files/named-classes.css">
  <link rel="stylesheet" type="text/css"
 href="research_files/manual.css">
  <link rel="stylesheet" type="text/css" href="research_files/form.css">
  <link rel="stylesheet" type="text/css"
 href="research_files/searchresults.css">
  <link rel="stylesheet" type="text/css"
 href="research_files/2007-header.css">
  <link rel="stylesheet" type="text/css"
 href="research_files/2007-layout.css">
  <link rel="stylesheet" type="text/css"
 href="research_files/2007-commonbase.css">
  <script type="text/javascript" src="research_files/gsd"></script>
  <script type="text/javascript" src="research_files/events"></script>
  <script type="text/javascript" src="research_files/dreamweaver"></script>
  <link rel="stylesheet" type="text/css" media="print"
 href="research_files/faculty_print.css">
<!-- #BeginEditable "doctitle" -->
  <title>Yoshua Bengio's Research</title>
<!-- #EndEditable --><!-- #BeginEditable "head" -->
  <style type="text/css">
<!--
.style1 {font-size: large}
-->
  </style><!-- #EndEditable -->
</head>
<body>
<div id="layout">
<div id="layout-header">
<div id="logo">
<table
 style="text-align: left; bottom: 0px; left: 0px; width: 382px; height: 90px; top: 0px;"
 border="0" cellpadding="0" cellspacing="0">
  <tbody>
    <tr>
      <td style="width: 50%; vertical-align: bottom;">
      <div id="diroAccueilLang"> <a
 href="http://www.iro.umontreal.ca/~lisa/lab"><img alt="LISA"
 src="research_files/title.jpg"
 style="border: 0px solid ; left: 2px; width: 192px; top: 9px; height: 63px;"></a>&nbsp;&nbsp;&nbsp;<br>
      </div>
      </td>
      <td style="vertical-align: bottom;"><a
 href="http://www.iro.umontreal.ca/~bengioy/yoshua_fr/recherche.html">&nbsp;Fr</a>&nbsp;&nbsp; <a
 href="research.html"><b>En</b></a><br>
      <br>
      </td>
    </tr>
  </tbody>
</table>
</div>
</div>
<div id="layout-menu">
<div id="subnavlinks">
<h1>YOSHUA BENGIO</h1>
<br>
<p><a href="index.html">Profile</a></p>
<p><a href="cv.html">Curriculum Vitae</a></p>
<p><a href="teaching.html">Teaching</a></p>
<p><a href="http://www.iro.umontreal.ca/~lisa/publications/index.php?page=author&amp;kind=single&amp;ID=1">Publications</a></p>
<p><a href="research.html"><font color="red">Research</font></a></p>
<p><a href="students.html">Students</a></p>
<p><a href="talks.html">Talks</a></p>
</div>
<div id="related">
<h1>Related Links</h1>
<br>
<p><a href="http://www.umontreal.ca/english/index.htm">University</a></p>
<p><a href="http://www.iro.umontreal.ca/?lang=en">Department</a></p>
<p><a href="http://www.iro.umontreal.ca/~lisa">Lab</a></p>
<p><a href="http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/PublicCode">Code</a></p>
</div>
<div id="address">
<h1>Hot Stuff</h1>
<br>
<p>
More <a href="Highlights.html">research highlights and selected recent papers</a>
<p>
<a href="http://www.iro.umontreal.ca/~bengioy/dlbook">
Deep Learning</a> - an MIT Press book in preparation
<p>
<p style="background-color: rgb(255, 204, 204); color: rgb(153, 51, 153);">
2012 Review paper on 
<a href="http://arxiv.org/abs/1206.5538">
unsupervised feature learning and deep learning</a>
</p>
<p style="background-color: rgb(255, 204, 204); color: rgb(153, 51, 153);">
2012 review paper on 
<a href="http://arxiv.org/abs/1206.5533">
Practical recommendations for gradient-based training of deep architectures</a>
</p>
<p>
Recreational speculations about 
intelligence and humanity as a 
<a href="research_files/collective.html">
collective learning system
</a>
</p>
</div>

<div id="address">
</div>
</div>
<div id="layout-main">
<div id="layout-main-content">
<p>&nbsp;</p>
<table border="0" cellpadding="6" cellspacing="0" width="589">
  <tbody>
    <tr valign="top">
      <td width="201"> <img alt="Yoshua Bengio"
 src="research_files/idlarge.jpg" style="width: 221px; height: 281px;"></td>
      <td width="50">&nbsp;</td>
      <td width="350"><!--page heading-->
      <p><img src="research_files/faculty_title.gif" border="0"></p>
      <div class="facultyName"><!--faculty name--> <br>
      <b>Yoshua Bengio</b><br>
Full Professor <br>
Department of Computer Science and Operations Research<br>
Canada Research Chair in Statistical Learning Algorithms<br>
<span style="font-style: italic;">&lt;my first name&gt; (dot) &lt;my last name&gt; (at sign) umontreal (dot) ca</span><span style="font-style: italic;"></span>
      </div>
      </td>
    </tr>
  </tbody>
</table>
<p>&nbsp;</p>
<!-- #BeginEditable "editable_content" -->
<p class="style1">&nbsp;</p>
<span class="facultytitle style1"><b><big>Research</big><br>
<br>
</b></span>
<p><span class="facultytitle style1"></span></p>

<table style="text-align: left; width: 782px; left: 223px;" border="0"
 cellpadding="2" cellspacing="2">
  <tbody>
    <tr>
      <td style="vertical-align: top;"><span class="facultytitle style1"><small><small>My
long-term goal is to <span style="font-weight: bold;">understand
the mechanisms giving rise to intelligence</span>;
understanding the underlying principles would deliver artificial
intelligence, and I believe that learning algorithms are essential in
this quest.<br>
      </small></small></span>
      <p><span class="facultytitle style1"><small><small>Machine
learning
algorithms attempt to endow machines with the ability to capture
operational knowledge through examples, e.g., allowing a machine to
classify or predict correctly in new cases. Machine learning research
has been extremely successful in the past two decades and is now
applied in many areas of science and technology, some well known
examples including web search engines, natural language translation,
speech recognition, machine vision, and data-mining. Yet, machines
still seem to fall short of
even mammal-level intelligence in many respects. One of the remaining
frontiers of machine learning is to make sense of the data, i.e., to
disentangle the underlying factors of variation. This requires learning
complicated and highly-varying functions such as those that are necessary to perform
machine vision or natural language processing tasks at a level
comparable to humans (even a 2-year old).<br>
      </small></small></span></p>
      <p><span class="facultytitle style1"><small><small>See my lab's <a
 href="http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/ResearchVision">long-term
vision web page</a> for a broader introduction.<br>
An introductory discussion of recent and ongoing research is below. 
See the lab's 
<a href="http://www.iro.umontreal.ca/~lisa/publications/index.php?page=author&amp;kind=single&amp;ID=1">publications site</a> 
for a downloadable and complete bibliographic list of my
papers.<br>
<h1>See this <a href="Highlights.html">page for recent research 
highlights and selected papers</a>. See my most recent (in progress)
book on deep learning (as it unfolds, <a href="http://www.iro.umontreal.ca/~bengioy/dlbook">there</a>).
</h1>
      </small></small></span></p>
      </td>
      <td style="vertical-align: top;">&nbsp;&nbsp;&nbsp;<img
 style="width: 222px; height: 221px; top: 527px; left: 746px;" alt=""
 src="research_files/AI-brain.jpg">&nbsp;&nbsp; <br>
      </td>
    </tr>
  </tbody>
</table>
<p>
Much of my fundamental research is funded by the National Sciences
and Engineering Research Council of Canada<a href="http://www.nserc.ca/">(NSERC)</a>,
the <a href="http://www.chairs.gc.ca/">Canada Research Chairs</a>
and the Canadian Institute for Advanced Research
<a href="http://www.cifar.ca/">(CIFAR)</a>.
</p>
<p>

<p><span class="facultytitle style1"><small><small><br>
Below you will find a brief description of
selected topics, including pointers to selected
recent publications. More details can be found in
my 
<a href="http://www.iro.umontreal.ca/~lisa/publications/index.php?page=author&amp;kind=single&amp;ID=1">publications site</a>:<br>
</small></small></span></p>
<ul>
  <li><a href="#need-deep">The need for deep architectures</a>: deep architectures are
a hot topic, why should we care? to be able to represent complicated functions compactly</li>
  <li><a href="#learning-deep">Learning algorithms for deep
architectures</a>: ok, we need deep architectures, but what how do we train them?</li>
  <li><a href="#non-local">The need for non-local generalization and
distributed representations</a>: depth is not enough, but is often combined with distributed representations.</li>
  <li><a href="#optimization">Strategies for non-convex optimization of
deep architectures</a>: training deep architectures presents a fundamental non-convex optimization challenge. 
Any hope for general principles to succeed training them?</li>
  <li><a href="#learning-context">Learning sequential dependencies and
language modeling</a>: we'll need to address that to deal with data involving time (language, video, music, etc.)</li>
  <li><a href="#baby-AI">The Baby AI project</a>: putting it all together...</li>
  <li><a href="#auto-encoders">Unsupervised learning</a> is believed to be a key for future
  progress on deep learning towards AI, and comes with apparently intractable challenges.
  We study how models based on auto-encoders can be used both to learn features and as generative models,
  avoiding some of these issues, and potentially opening the door to methods that can disentangle
  the underlying factors of variation and do better credit assignment in
  supervised learning as well.
</ul>
<p><span class="facultytitle style1"></span></p>
<b></b> <br>
<table style="width: 100%; background-color: rgb(204, 255, 255);"
 border="1">
  <tbody>
    <tr>
      <td class="content">
      <p class="content"><b><a name="need-deep"></a><big>The Need for
Deep Architectures</big></b> </p>
      <p class="content">As described in an 2007 NSF report on <a
 href="http://www.cnl.salk.edu/Media/NSFWorkshopReport.v4.pdf">Future
Challenges for the Science and Engineering of Learning</a>, one of the
missing ingredients is <span style="font-weight: bold;">depth</span>. <span
 style="font-style: italic;">Deep learning methods</span> aim at
learning of feature hierarchies with features from higher-levels of the
hierarchy formed by the composition of lower level features.
Automatically learning features at multiple <span
 style="font-style: italic;">levels of abstraction</span> allows a
system to learn complex functions mapping the input to the output
directly from data, without depending completely on human crafted
features. This is especially important for higher-level abstractions,
which humans often do not know how to specify explicitly. <span
 style="font-style: italic;">Complexity theory</span> theorems suggest
that one of the missing ingredients in current learning algorithms is <span
 style="font-weight: bold;">depth of architecture</span> (the number of
levels of composition in the learned function, e.g. number of layers of
a neural network), illustrated below<br>
      </p>
      <table align="center" width="100">
        <tbody>
          <tr>
            <td align="center" valign="baseline"><span class="caption">&nbsp;</span><img
 style="width: 388px; height: 240px; top: 849px; left: 460px;" alt=""
 src="research_files/depth.jpg"><span class="caption">Illustration of
circuit depth, for two circuits built from different element sets.<br>
On the left the circuit implements x*sin(a*x+b).&nbsp; Depth is 4.<br>
On the right the circuit implements a neural nework with three layers.
Depth is 3.<br>
            </span></td>
          </tr>
        </tbody>
      </table>
      <p><br>
The most suggestive result is that a depth-k architecture could
represent a particular function
efficiently, a depth (k-1) architecture might need an exponential
number of components, and thus a huge number of training examples. See
these papers for a broader discussion:<br>
      </p>
      <ul>
        <li>Yoshua Bengio, <a
 href="http://www.iro.umontreal.ca/~lisa/publications/index.php?page=publication&amp;kind=single&amp;ID=239"><span
 style="font-weight: bold;">Learning deep architectures for AI</span>,</a>
To appear in <span
 style="font-style: italic;">Foundations and Trends for Machine Learning</span>.</li>
        <li>Yoshua Bengio and Yann Le Cun, <a
 style="font-weight: bold;"
 href="http://www.iro.umontreal.ca/~lisa/publications/index.php?page=publication&amp;kind=single&amp;ID=4">Scaling
Learning Algorithms towards AI</a>, in: <span
 style="font-style: italic;">Large Scale Kernel Machines</span>, MIT
Press, 2007.<br>
        </li>
      </ul>
      <span class="content">&nbsp;&nbsp;&nbsp; </span></td>
    </tr>
  </tbody>
</table>
<p>&nbsp;<br>
</p>
<table
 style="text-align: left; width: 100%; background-color: rgb(255, 204, 204);"
 border="1" cellpadding="1" cellspacing="1">
  <tbody>
    <tr>
      <td
 style="vertical-align: top; background-color: rgb(255, 204, 204);"><b><a
 name="learning-deep"></a><big>Learning Algorithms for Deep
Architectures</big><br>
      </b><br>
Until 2006, attempts at training deep architectures (e.g. multi-layer
neural networks with more than 2 hidden layers) were unsuccessful.
Since then, several strategies have been proposed and successfully
demonstrated to train deeper architectures. A prominent example of such
algorithms are the <a
 href="http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/DeepBeliefNetworks"><span
 style="font-weight: bold;">Deep Belief Networks</span></a>, which are
graphical models structured as stochastic neural networks with many
layers, and that can be trained in an unsupervised way, one layer at a
time. <br>
      <table
 style="text-align: left; margin-left: auto; margin-right: auto;">
        <tbody>
          <tr>
            <td class="content" style="text-align: center;"><img
 style="width: 150px; height: 206px; top: 1973px; left: 496px;" alt=""
 src="research_files/DBN1.jpg"><img
 style="width: 144px; height: 198px; top: 2289px; left: 496px;" alt=""
 src="research_files/DBN2.jpg"><img
 style="width: 128px; height: 179px; top: 1996px; left: 528px;" alt=""
 src="research_files/DBN3.jpg"> <span class="caption"><br>
Training and initializing each layer of a 3-layer Deep Belief Network
as a Restricted Boltzmann Machine (RBM).<br>
The RBM is an unsupervised model of its input, which&nbsp; is the
output of the previous layer. Each layer represents <br>
a higher level abstraction, a more non-linear (stochastic)
transformation of the raw input x.<br>
            </span></td>
          </tr>
        </tbody>
      </table>
      <br>
Each layer represents a factor model for the layer below, and
altogether we obtain a highly non-linear factor model. Since 2006, the
number of papers on deep architectures has grown very quickly, several
other algorithms for deep architectures have been proposed, exciting
experimental results have been obtained on a wide variety of tasks, and
funding agencies are starting to perceive the importance of this
research question. Hinton et al introduced Deep Belief Networks in a <a
 href="http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf">2006
Neural Computation paper</a>. I wrote the following review paper&nbsp;
on the motivations and algorithms for deep architectures:<br>
      <ul>
        <li>Yoshua Bengio, 
<a href="http://www.iro.umontreal.ca/~lisa/publications/index.php?page=publication&amp;kind=single&amp;ID=239">
<span style="font-weight: bold;">Learning deep architectures for AI</span>,</a>
To appear in <span
 style="font-style: italic;">Foundations and Trends for Machine Learning</span>.</li>
      </ul>
From our lab, these other papers can be considered follow-up work on
Hinton et al's foundational work on RBMs and DBNs:<br>
      <ul>
        <li>Yoshua Bengio, Pascal Lamblin, Dan Popovici and Hugo
Larochelle, <a
 href="http://www.iro.umontreal.ca/~lisa/publications/index.php?page=publication&amp;kind=single&amp;ID=190"><span
 style="font-weight: bold;">Greedy Layer-Wise Training of Deep Networks</span></a>,
in: Advances in Neural Information Processing Systems 19 (NIPS'2006),
pages 153-160, MIT Press, 2007.</li>
        <li>Hugo Larochelle, Dumitru Erhan, Aaron Courville , James
Bergstra and Yoshua Bengio, <a
 href="http://www.iro.umontreal.ca/~lisa/publications/index.php?page=publication&amp;kind=single&amp;ID=193">An
Empirical Evaluation of Deep Architectures on Problems with Many
Factors of Variation</a>, in: ICML'2007, pages 473-480, 2007.<br>
        </li>
        <li>Hugo Larochelle and Yoshua Bengio, <a
 style="font-weight: bold;"
 href="http://www.iro.umontreal.ca/~lisa/publications/index.php?page=publication&amp;kind=single&amp;ID=216">Classification
using Discriminative Restricted Boltzmann Machines</a>, in:
International Conference on Machine Learning proceedings (ICML'2008),
2008.</li>
        <li>Nicolas Le Roux and Yoshua Bengio, <a
 href="http://www.iro.umontreal.ca/~lisa/publications/index.php?page=publication&amp;kind=single&amp;ID=199"><span
 style="font-weight: bold;">Representational Power of Restricted
Boltzmann Machines and Deep Belief Networks</span></a>, in: Neural
Computation, volume to appear, 2008.</li>
        <li>Pascal Vincent, Hugo Larochelle, Yoshua Bengio and
Pierre-Antoine Manzagol , <a
 href="http://www.iro.umontreal.ca/~lisa/publications/index.php?page=publication&amp;kind=single&amp;ID=217"><span
 style="font-weight: bold;">Extracting and Composing Robust Features
with Denoising Autoencoders</span></a>, in: International Conference on
Machine Learning proceedings (ICML'2008), 2008.<br>
        </li>
      </ul>
      <br>
      </td>
    </tr>
  </tbody>
</table>
<br>
<br>
<table
 style="text-align: left; width: 100%; background-color: rgb(255, 204, 255);"
 border="1" cellpadding="1" cellspacing="1">
  <tbody>
    <tr>
      <td style="vertical-align: top;"><b><a name="non-local"></a><big>The
Need for Non-Local Generalization and Distributed Representations</big><br>
      <br>
      </b>In addition to depth of architecture, we have found that
another ingredient is crucial: <span style="font-style: italic;">distributed
representations</span>. We and others have found that most
non-parametric learning algorithms suffer from the so-called <span
 style="font-style: italic; font-weight: bold;">curse of dimensionality</span>.
We would prefer to call it the curse of highly-varying functions or the
curse of locality. That curse occurs when the only way a learning
algorithm generalizes to a new case x is by exploiting only a raw
notion of similarity (such as Euclidean distance) between the cases.
This is typically done by the learner looking in its training examples
for cases that are close to x according to some similarity measure. One
can often interpret what these algorithms do as some kind of
interpolation between the neighboring examples. Imagine trying to
approximate a function by many small linear or constant pieces. We need
at least one example for each piece. We can figure out what each piece
should look like by looking mostly at the examples in the neighborhood
of each piece. If the target function has a lot of variations, we'll
need correspondingly many training examples. In dimension d (or on a
manifold of dimension d), the number of variations may grow
exponentially with d, hence the number of required examples. However,
if we are lucky, we may still obtain good results when we are trying to
discriminate between two highly complicated regions (manifolds), e.g.
associated with two classes of objects. Even though each manifold may
have many variations, they might be separable by a smooth (maybe even
linear) decision surface. That is the situation where local
non-parametric algorithms work well. They also work comparatively well
when the distribution of examples is very noisy, because it is
difficult to capture much signal then (and a smooth predictor is pretty
much the best one can do).<br>
      <br>
Distributed representations are transformations of the data that
compactly capture many different factors of variations present in the
data. Because many examples can inform us about each of these factors,
and because each factor may tell us something about examples that are
very far from the training examples, it is possible to generalize
non-locally, and escape the curse of dimensionality. The simplest
distributed representation is a linear transformation of the input
example (e.g., learned by Principal Components Analysis, Partial Least
Squares, Independent Component Analysis or other more recent
techniques, often called Factor Models). To represent more complicated
functions, we need non-linear factor models, such as the Restricted
Boltzmann Machine and Auto-Encoder introduced as building blocks of
deep architectures.<br>
      <br>
      <table style="text-align: left; width: 100%;" border="0"
 cellpadding="2" cellspacing="2">
        <tbody>
          <tr>
            <td style="vertical-align: top;"><img
 style="width: 283px; height: 192px; top: 2791px; left: 229px;" alt=""
 src="research_files/local.jpg"></td>
            <td style="vertical-align: top;"><img
 style="width: 257px; height: 250px; top: 2633px; left: 526px;" alt=""
 src="research_files/CurseDimensionality.jpg"><br>
            </td>
            <td style="vertical-align: top;"><img
 style="width: 249px; height: 196px; top: 2699px; left: 751px;" alt=""
 src="research_files/multi-partitions.jpg"><br>
            </td>
          </tr>
          <tr>
            <td style="vertical-align: top;"><span class="caption">Generalizing
locally is easy: it amounts to interpolating between neighboring
training examples (starred points, above). In fact, local learning
algorithms (generally kernel machines) work very well in low dimension
or when the function to capture can be described with a smooth
manifold. But when the target function has many ups and downs, one
needs at least that many examples.</span><br>
            </td>
            <td style="vertical-align: top;"><span class="caption">However,
in high dimension, the neighborhood becomes exponentially large, and
one requires an exponential number of training examples to cover it. To
cover and discriminate among&nbsp; N regions in input space, one would
need O(N) examples with a local learning algorithm, but N can grow to
the power fo the space dimension.<br>
            </span></td>
            <td style="vertical-align: top;"><span class="caption">Instead,
input variations can sometimes be compactly described using a
distributed representation. Above, each hyper-plane creates a 2-way
partition of the space, and K hyper-planes (= K linear classifiers = K
hidden units of a neural net) can capture and discriminate among 2**K
regions in input space.<br>
            </span></td>
          </tr>
        </tbody>
      </table>
      <br>
This is an early paper on fighting the curse of dimensionality in
high-dimensional discrete distributions using neural networks:<br>
      <ul>
        <li>Samy Bengio and Yoshua Bengio, <a
 href="http://www.iro.umontreal.ca/~lisa/publications/index.php?page=publication&amp;kind=single&amp;ID=74"><span
 style="font-weight: bold;">Taking on the Curse of Dimensionality in
Joint Distributions Using Neural Networks,</span></a> in: <span
 style="font-style: italic;">IEEE Transaction on Neural Networks</span>,
special issue on data mining and knowledge discovery, volume 11, number
3, pages 550-557, 2000.</li>
      </ul>
And this one an attempt to introduce a non-local component in a
non-parametric density estimation algorithm by using a neural net to
predict the manifold structure at each point x, as a function of x:<br>
      <ul>
        <li>Yoshua Bengio, Hugo Larochelle and Pascal Vincent, <a
 href="http://www.iro.umontreal.ca/~lisa/publications/index.php?page=publication&amp;kind=single&amp;ID=18"><span
 style="font-weight: bold;">Non-Local Manifold Parzen Windows</span></a>,
in: Advances in Neural Information Processing Systems 18 (NIPS'2005),
MIT Press, 2006.<br>
        </li>
      </ul>
Our more recent work has focussed on demonstrating mathematically the
limitations of modern non-parametric learning algorithms such as SVMs,
Gaussian Processes, decision trees, and manifold learning algorithms
based on the neighborhood graph:<br>
      <ul>
        <li>Yoshua Bengio, Olivier Delalleau and Nicolas Le Roux, <a
 style="font-weight: bold;"
 href="http://www.iro.umontreal.ca/~lisa/publications/index.php?page=publication&amp;kind=single&amp;ID=17">The
Curse of Highly Variable Functions for Local Kernel Machines</a>, in:
Advances in Neural Information Processing Systems 18 (NIPS'2005), pages
107-114, MIT Press, 2006.</li>
        <li>Yoshua Bengio, Martin Monperrus and Hugo Larochelle, <a
 style="font-weight: bold;"
 href="http://www.iro.umontreal.ca/~lisa/publications/index.php?page=publication&amp;kind=single&amp;ID=15">Non-Local
Estimation of Manifold Structure</a>, in: Neural Computation, volume
18, pages 2509-2528, 2006.<br>
        </li>
        <li>Yoshua Bengio, Olivier Delalleau and Clarence Simard, <a
 style="font-weight: bold;"
 href="http://www.iro.umontreal.ca/~lisa/publications/index.php?page=publication&amp;kind=single&amp;ID=198">Decision
Trees do not Generalize to New Variations</a>, technical report number
1304, 2007.</li>
        <li>Yoshua Bengio and Yann Le Cun, <a
 style="font-weight: bold;"
 href="http://www.iro.umontreal.ca/~lisa/publications/index.php?page=publication&amp;kind=single&amp;ID=4">Scaling
Learning Algorithms towards AI</a>, in: <span
 style="font-style: italic;">Large Scale Kernel Machines</span>, MIT
Press, 2007.</li>
      </ul>
      <br>
      <br>
      <b><br>
      </b></td>
    </tr>
  </tbody>
</table>
<p><br>
</p>
<p><br>
</p>
<table
 style="text-align: left; width: 100%; background-color: rgb(255, 255, 204); margin-left: auto; margin-right: auto;"
 border="1" cellpadding="2" cellspacing="2">
  <tbody>
    <tr>
      <td style="vertical-align: top;"><big><span
 style="font-weight: bold;"><a name="optimization"></a>Strategies for
Non-Convex Optimization of Deep Architectures</span></big><br>
      <br>
      <br>
One basic hypothesis of our work is that the main stumbling block
explaining past failures at training deep architectures is due to an
optimization difficulty: learning with deep architectures involves a
difficult optimization problem, whose exact solution is intractable.
The optimization problem is made difficult because of the presence of
local minima and plateaus. However, it is encouraging to note that
animals and humans seem to apply sub-optimal but wholly adequate
strategies (that we have yet to fully elucidate!). The optimization
problem is made difficult because of the presence of local minima and
plateaus. The working strategies we know of involve at least one of
these ingredients:<br>
      <ol>
        <li>guiding the optimization or</li>
        <li>the exploration of many solutions (regions in function
space) in parallel.</li>
      </ol>
Since deeper networks are more difficult to train than shallow ones, it
is not surprising to find that many of the 'guiding' strategies can be
seen as giving hints to the hidden layers, to help them move to regions
near good solutions to the learning task. It is conjectured that in
order to generalize to a wide array of tasks (e.g. all related to
vision), unsupervised and semi-supervised learning (using mostly as
data examples that have not been hand-labeled) is crucial. One way to
unify many successful strategies for training deep architectures is to
observe that they exploit the idea that one can get near a good
solution to the optimization problem by considering a smoother training
criterion, and gradually making it closer to the target criterion of
interest, tracking a local minimum along the way (this is called a
continuation method, illustrated below).<br>
&nbsp; <br>
      <table
 style="text-align: left; width: 45%; margin-left: auto; margin-right: auto;"
 border="1" cellpadding="1" cellspacing="1">
        <tbody>
          <tr>
            <td style="vertical-align: top;"><img
 style="width: 357px; height: 253px;" alt=""
 src="research_files/continuation.jpg"><br>
            </td>
          </tr>
        </tbody>
      </table>
      <br>
Our research objective is to further improve our understanding of these
algorithms and to exploit it to develop new strategies for training
deep architectures, taking inspiration from the natural world (how
humans manage to learn such complicated tasks). An example of
inspiration from the natural world comes from the way in which children
develop and learn new skills. Whereas machine learning algorithms are
typically provided with a single homogeneous training set of examples,
children learn in stages, moving to a next stage only after having
mastered the previous stage, and they care mostly about examples
illustrating the concepts that are on the frontier of their
understanding of the world. That is why schooling is normally organized
in the form of a curriculum. This principle is also called shaping in
the world of animal training, where a sequence of gradually more
difficult tasks is set up for an animal.<br>
      <br>
This is a new research thread in my lab, and the only published work
discussing these ideas are the following. First, we did experiments
that confirm that the main issue with the traditional optimization
method for deep neural networks is with the difficulty in optimizing
the lower layers. We also show the importance of the <span
 style="font-style: italic;">unsupervised</span> layer-wise
inititialization (by opposition to supervised):<br>
      <ul>
        <li>Yoshua Bengio, Pascal Lamblin, Dan Popovici and Hugo
Larochelle, <a
 href="http://www.iro.umontreal.ca/~lisa/publications/index.php?page=publication&amp;kind=single&amp;ID=190"><span
 style="font-weight: bold;">Greedy Layer-Wise Training of Deep Networks</span></a>,
in: Advances in Neural Information Processing Systems 19 (NIPS'2006),
pages 153-160, MIT Press, 2007.</li>
      </ul>
In this paper we summarize several of the above ideas on strategies
that have been or may be employed to circumvent the optimization
difficulty of deep architectures:<br>
      <ul>
        <li>Yoshua Bengio, <a
 href="http://www.iro.umontreal.ca/~lisa/publications/index.php?page=publication&amp;kind=single&amp;ID=299"><span
 style="font-weight: bold;">Learning deep architectures for AI</span>,</a>
To appear in <span
 style="font-style: italic;">Foundations and Trends for Machine Learning</span>.</li>
      </ul>
      <br>
      </td>
    </tr>
  </tbody>
</table>
<p><br>
</p>
<table
 style="text-align: left; width: 100%; background-color: rgb(204, 204, 255);"
 border="1" cellpadding="1" cellspacing="1">
  <tbody>
    <tr>
      <td style="vertical-align: top;"><big style="font-weight: bold;"><a
 name="learning-context"></a>Learning sequential dependencies and
language modeling</big><br>
      <br>
One of the highest impact results I obtained was about the difficulty
of learning sequential dependencies, either in recurrent neural
networks or in dynamical graphical models (such as Hidden Markov
Models). The paper below suggests that with parametrized dynamical
systems (such as a recurrent neural network), the error gradient
propagated through many time steps is a poor source of information for
learning to capture statistical dependencies that are temporally
remote. The mathematical result is that either information is not
easily transmitted (it is lost exponentially fast when trying to
propagate it from the past to the future through a context variable, or
it is vulnerable to perturbations and noise), or the gradients relating
temporally remote events becomes exponentially small for larger
temporal differences.<br>
      <ul>
        <li>Yoshua Bengio, Patrice Simard and Paolo Frasconi, <a
 href="http://www.iro.umontreal.ca/~lisa/publications/index.php?page=publication&amp;kind=single&amp;ID=144"><span
 style="font-weight: bold;">Learning Long-Term Dependencies with
Gradient Descent is Difficult</span></a>, in: IEEE Transactions on
Neural Networks, volume 5, number 2, pages 157-166, 1994.</li>
      </ul>
Similar results were shown for HMMs there:<br>
      <ul>
        <li>Yoshua Bengio and Paolo Frasconi, <a
 href="http://www.iro.umontreal.ca/~lisa/publications/index.php?page=publication&amp;kind=single&amp;ID=129"><span
 style="font-weight: bold;">Diffusion of Credit in Markovian Models</span></a>,
in: Advances in Neural Information Processing Systems 7 (NIPS'1994),
pages 553-560, MIT Press, Cambridge, MA, 1995.</li>
      </ul>
Is there any hope? Humans seem to manage to learn through many time
scales. This has inspired this paper:<br>
      <ul>
        <li>Salah El Hihi and Yoshua Bengio, <a
 href="http://www.iro.umontreal.ca/~lisa/publications/index.php?page=publication&amp;kind=single&amp;ID=125"><span
 style="font-weight: bold;">Hierarchical Recurrent Neural Networks for
Long-Term Dependencies</span></a>, in: Advances in Neural Information
Processing Systems 8, MIT Press, Cambridge, MA, 1996.</li>
      </ul>
      <br>
Our recent work with unsupervised learning for deep architectures, Mnih
and Hinton's (ICML'2007) work on temporal RBMs, unpublished work by
James Bergstra (thesis proposal), and unpublished work by Ilya
Sutskever (U. Toronto), all suggest that there may be ways around the
issues introduced in the above papers in the mid-90's. Exploring these
potential solutions is one of the current undertakings in my lab.<br>
&nbsp;j<br>
      <table style="text-align: left; width: 100%;" border="1"
 cellpadding="2" cellspacing="2">
        <tbody>
          <tr>
            <td style="vertical-align: top;">Sequential dependencies
are particularly important for modeling the
statistical structure of language. We have been working on such models
and continue to do so. Our early work was meant to show the advantage
of using distributed representations to beat state-of-the-art
statistical language models (smoothed n-grams):<br>
            <ul>
              <li>Yoshua Bengio, Réjean Ducharme, Pascal Vincent and
Christian Jauvin, <a
 href="http://www.iro.umontreal.ca/~lisa/publications/index.php?page=publication&amp;kind=single&amp;ID=49"><span
 style="font-weight: bold;">A Neural Probabilistic Language Model</span></a>,
in: Journal of Machine Learning Research, volume 3, pages 1137-1155,
2003.</li>
            </ul>
This work has been followed up by many, as discussed in this
scholarpedia survey article:<br>
            <ul>
              <li>Yoshua Bengio, <a
 href="http://www.iro.umontreal.ca/~lisa/publications/index.php?page=publication&amp;kind=single&amp;ID=210"><span
 style="font-weight: bold;">Neural net language models</span></a>, in:
Scholarpedia, volume 3, number 1, pages 3881, 2008.</li>
            </ul>
To speed-up these models, several approaches were explored:<br>
            <ul>
              <li>Frederic Morin and Yoshua Bengio, <a
 href="http://www.iro.umontreal.ca/~lisa/publications/index.php?page=publication&amp;kind=single&amp;ID=20"><span
 style="font-weight: bold;">Hierarchical Probabilistic Neural Network
Language Model,</span></a> in: Proceedings of the Tenth International
Workshop on Artificial Intelligence and Statistics, pages 246-252,
Society for Artificial Intelligence and Statistics, 2005.</li>
              <li>Yoshua Bengio and Jean-Sébastien Sénécal, <a
 href="http://www.iro.umontreal.ca/~lisa/publications/index.php?page=publication&amp;kind=single&amp;ID=205"><span
 style="font-weight: bold;">Adaptive Importance Sampling to Accelerate
Training of a Neural Probabilistic Language Model</span></a>, in: IEEE
Transactions on Neural Networks, volume 19, number 4, pages 713-722,
2007.<br>
              </li>
            </ul>
            </td>
            <td style="vertical-align: top;"><img
 style="width: 371px; height: 337px; top: 4674px; left: 619px;" alt=""
 src="research_files/Neural_Net_Language_Model_direct.jpg"><br>
            </td>
          </tr>
        </tbody>
      </table>
      <br>
      </td>
    </tr>
  </tbody>
</table>
<p><br>
</p>
<table
 style="text-align: left; width: 100%; background-color: rgb(255, 153, 153);"
 border="1" cellpadding="1" cellspacing="1">
  <tbody>
    <tr>
      <td style="vertical-align: top;"><big><span
 style="font-weight: bold;"><a name="baby-AI"></a>The Baby AI Project</span></big><br>
      <br>
The Baby AI project is a way to make concrete the above research, with
the additional insight that a major source of data about "the world of
humans" comes from human-generated productions such as videos,
television shows, or web pages, and that images (or image sequences)
and natural language (in the form of textual captions or of sound) are
the main modalities involved. In addition, any AI will presumably
communicate with humans through these modalities, an AI will need to
master these modalities. Furthermore, it seems obvious these modalities
reinforce each other in the process of extracting the regularities we
try to learn.<br>
      <br>
      <table style="text-align: left; width: 100%;" border="1"
 cellpadding="2" cellspacing="2">
        <tbody>
          <tr>
            <td style="vertical-align: top;"><img
 style="width: 606px; height: 206px; top: 4567px; left: 297px;" alt=""
 src="research_files/curriculum_illustr.jpg"><br>
            <span class="caption">The objective is to train our Baby AI
with a series of increasingly complex scenes illustrating gradually
more abstract concepts, rooted in the main modalities human use to
communicate (images, video, language).</span><br>
            </td>
            <td style="vertical-align: top;"><img
 style="width: 181px; height: 211px; top: 4600px; left: 880px;" alt=""
 src="research_files/guide-baby-walk.jpg"><br>
            <span class="caption">Our research suggests that guiding
the optimization is crucial to allow learning of complex abstractions.</span><br>
            </td>
          </tr>
        </tbody>
      </table>
      <br>
In the short term we want to explore the limitations of current
algorithms for deep architectures through a series of gradually more
complex artificial worlds from which we generate examples. There are
two advantages to this "starting small" strategy. Firstly, it is easier
to understand what works and does not work when the setting and the
concepts to learn are simpler. Second, once our algorithms have
mastered a set of concepts, it makes sense to apply them to more
complex ones and we can even initialize the new models using the models
trained on the simpler concepts. In the medium term, we would like to
add to virtual data (for which we know the underlying true semantics)
unlabeled data from a very rich source such as TV programs.<br>
Some of the basic research questions that we explore in this setting
are the following:<br>
      <ul>
        <li>Learning features reduces the need for human effort in
crafting useful representations of the data. Researchers in fields such
as computer vision have spent considerable energy engineering useful
feature sets for various tasks and as new tasks emerge more effort will
be required to devise new ones. The demonstrated ability of deep
learning methods to automatically learn feature-sets from data has the
potential to greatly expand the rate of progress in existing
applications and could dramatically increase the rate of emergence of
new applications for machine learning. We want to explore more learning
algorithms that extract explanatory factors or features, as they are
important building blocks.</li>
      </ul>
      <ul>
        <li>Learning features from vast quantities of data has the
potential to develop features of greater complexity (at higher levels
of the hierarchy) than is achievable by human engineering. These
features, reflecting invariances extant in the data, can increase the
performance and robustness of a system that uses these features, as has
already been confirmed by us and others, not only in classification
tasks, but also in regression, dimensionality reduction, modeling
textures, language modeling, information retrieval, robotics, and
collaborative filtering.</li>
      </ul>
      <ul>
        <li>Context representation: we are trying to generalize the
deep architectures so they represent and learn the dynamics, the
context; this is related to older research on long term dependencies
and about generalizing probabilistic graphical models with distributed
representations.</li>
      </ul>
      <ul>
        <li>Many levels of abstraction, temporal hierarchy: the need to
represent events at many time scales, with finer scales defining
hierarchically what happens at coarser scales.</li>
      </ul>
      <ul>
        <li>Multi-modality with partial observability: in some cases we
only observe images, in others only text and sound, etc. We would like
our algorithms to be able to infer "explanations" about the input that
can take into account the modalities that are available in each
particular case. This implies forward propagating paths for each
modality that can be disjoint or connected, according to the need.</li>
      </ul>
      <ul>
        <li>Asynchronicity: the image and text streams are completely
asynchronous. How to unify them in the perspective of learning the
joint between the two.</li>
      </ul>
      <ul>
        <li>Computational efficiency: the images coming from a video
can be quite big. So is the vocabulary. We would like to process them
in a reasonable time, so the learning time per frame is of the order of
the inter-frame time. This might imply parallelism, or implementation
optimizations, or special hardware.</li>
      </ul>
      </td>
    </tr>
  </tbody>
</table>
<p><br>
</p>
<table
 style="text-align: left; width: 100%; background-color: rgb(255, 193, 205);"
 border="1" cellpadding="1" cellspacing="1">
  <tbody>
    <tr>
      <td style="vertical-align: top;"><big><span
 style="font-weight: bold;"><a name="baby-AI"></a>Unsupervised Learning: disentangling the underlying factors of variation</span></big><br>
      <br>
Unsupervised learning attempts to discover structure in the data, and in its most general form
that means that the joint distribution between the observed variable is captured. It may then
be possible to answer an exponential number of questions about these variables, e.g., 
what configurations of some variables are likely
given values of other variables?<br>
Whereas estimating a probability function by maximum likelihood is statistically efficient
(with the right model structure) it poses intractable computational challenges, which
are revealed when studing various graphical models such as Boltzmann machines and directed
graphical models.<br> We attempt to better understand these issues and either find solutions
within this framework or explore completely different approaches to unsupervised learning,
based on auto-encoders.
      <br>
See this <a href="http://arxiv.org/abs/1407.7906">technical report on auto-encoders and target-propagation</a> 
for a recent proposal to use auto-encoders to obtain
both tractable deep generative models and potentially replace back-prop as a means
to do credit assignment through deep and strong non-linearities.
<!--
      <table style="text-align: left; width: 100%;" border="1"
 cellpadding="2" cellspacing="2">
        <tbody>
          <tr>
            <td style="vertical-align: top;"><img
 style="width: 606px; height: 206px; top: 4567px; left: 297px;" alt=""
 src="research_files/curriculum_illustr.jpg"><br>
            <span class="caption">The objective is to train our Baby AI
with a series of increasingly complex scenes illustrating gradually
more abstract concepts, rooted in the main modalities human use to
communicate (images, video, language).</span><br>
            </td>
            <td style="vertical-align: top;"><img
 style="width: 181px; height: 211px; top: 4600px; left: 880px;" alt=""
 src="research_files/guide-baby-walk.jpg"><br>
            <span class="caption">Our research suggests that guiding
the optimization is crucial to allow learning of complex abstractions.</span><br>
            </td>
          </tr>
        </tbody>
      </table>
      <br>
<-->
    </tr>
  </tbody>
</table>
<p><br>
</p>
<!-- #EndEditable --> </div>
<p> </p>
</div>
</div>
<!-- #EndTemplate -->
</body>

<!-- Mirrored from www.iro.umontreal.ca/~bengioy/yoshua_en/research.html by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 14 May 2016 07:36:38 GMT -->
</html>
